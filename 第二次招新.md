# Resnet
1,问题：我们面临的一个问题是更深的神经网络难以训练；向一个本来就比较深的模型加入更多层会导致更高的误差；但是越深的神经网络越复杂，能够表达的信息，包含的内容就更多；因此这成为一个急需解决的问题。
2，前人已经在其他领域证实，求解残差 相比 直接求解来说可能更好；
   另一方面说明维度进行投影映射地合理性

3，引入残差学习，每一个模块最后的形式为 y = f(x) + x ;  f(x) 为我们想要学习的内容 ； 当遇到维度不匹配的时候，通过全连接层与卷积层的映射进行对齐；

4，对比对照实验

4.1 在imagenet数据集上进行评估，plain networks(18-layer,34-layer),residual networks进行对比 ，说明了以下几个问题
1）普通模型加深层数会导致退化问题。
2）resnet在层数较少效果不明显，但是会加快收敛
3）在较深的模型上面有明显效果。
此外，还对比进行映射对齐维度对于训练的影响：
对比 （A）零填充，（B）恒等 （C）均为投影 整体对于训练的效果影响不大，差异很小；但是由于复杂度的影响还是会采用恒等的形式；
后续继续尝试加深，仍未观察到退化问题。

4.2 CIFAR-10  普通网络和残差网络进行对比；和其他数据集的现象相似；

探索极深网络，能够收敛，但是会严重过拟合（感觉是数据太少的问题）

层响应分析：展示各层响应的标准差，
1）残差函数普遍小于非残差函数；
2）随着模型的深度的增加，响应的幅度变得更小，说明网络的状态很稳定

4.3 在其他任务上验证，效果良好

5，不足，极深的网络仍会存在过拟合。
   当模型训练较好的时候，有些模块可以认为权重学习接近为0；对于这个层地本身来说，是一种浪费；
   由于有两条路，反向传播时候梯度可能只有少部分通过残差块

# ViT
1，问题：Transformer在语言处理方面的应用广泛，希望它能够直接迁移到图像处理方面，

2，CNN先将特征图变为更小，然后展开，进行自注意力；
  孤立自注意力，提取局部的窗口，进行自注意力；
  轴注意力，高度和宽度都进行自注意力，硬件加速问题；

  有一篇很雷同的paper，处理方式相同；

3，作者希望的是能够直接将传统的Transformer直接进行应用到图片处理，因此可以认为是对图片进行预处理，
首先将图片切分成一个一个的patch，每个patch展平，然后降维，加位置编码，加[class]token/也可以直接进行投影。   
1）不存在归纳偏置，并不存在类似cnn的先验信息，因此需要从0开始学习，这在一定程度上影响了ViT在中小规模数据集上面的表现。
2）位置编码为一维；2D编码结果类似；  进行消融实验，有无和不同的位置编码
3）只采用第0个token分类，是为了符合原始的token

微调与更高分辨率：用更高分辨率的图片微调，模型效果更好；
位置信息插值，这个对于效果会有影响

4，对比对照实验，不同模型的对比以及数据集大小间的对比

4.3预训练的影响；
1)不同规模数据集，以及大小不一的网络，随着数据集规模的增大，较大的vit效果逐步提升;
2)在 JFT-300M 的随机子集上训练,评估效果，数据集较小时，ViT效果不是很好，随着数据集增大，效果变好
极低样本的数据迁移很有潜力

4.5
1D位置编码效果已经很好，相似性图看出，已经学到2D的概念
ViT自注意力图可以看出，网络一开始就可以看到较远的像素点；
输出token折射到输入的图片，模型已经关注到我们真正想要的/

4.6 自监督预训练进行探索，有很好的效果，但是相比于有监督预训练效果差。

5
好：将标准的Transformer编码器可以直接应用到图像上面，可以直接使用最传统的transformer；
   实验的对比对照部分，十分详细，充分验证了Transformer对于数据的依赖，以及相比于其他模型的效果，
   结果可以迁移
缺点：对数据集的规模依赖很大，本身的归纳偏置不如CNN；
     只是针对分类，对于其他没有涉及（例如检测，分割）；

# CLIP
1，传统的训练 固定数据集类别，标签，数据集同样需要需要手工标注，限制了模型的泛化性，这对于迁移来说不是很方便；
   nlp采用自监督训练，效果很好，希望能够用到视觉上面，但是这个过程有涉及到视觉数据和文本数据的特性不同等问题
   较为困难。

2，Visual N-Grams   生成式训练；预测标题的词；但是这个方法的话，由于同一个词的歧义性，导致可能会影响精度
   VirTex   图像编码器CNN,文本解码器ViT  给定图片样本，输出合理的语言描述    
   ICMLM     掩码语言建模   和上一篇的问题类似，在这个过程中需要语言模型来转换为语言特征，作为代理任务，这个对于视觉部分来说舍本逐末
   ConVIRT  这篇主要是针对医学图像的一片论文，方法与文章相似

3， 自然语言处理监督信号训练可迁移的视觉模型 
   预训练：图片和文本都有编码器，编码后的结果进行对比学习（正负样本） 编码器本身不需要进行预训练，不会过拟合，且由于数据很多，不需要用到数据增强
   选择模型：模型尝试多种包括resnet和vit，与大小成正相关 
   分类： 分类图片特征和文本特征计算相似性，从而进行分类
   语义性强，迁移效果好
   训练和推理的时候，可以放入新的类别

4, 1）CLIP 和 Visual N-Grams 进行对比；  说明了数据的重要性

   prompt engineering and ensembling  语义的歧义性；且训练的时候同样为句子，对于准确度来说会有一定的提高.
   提示词可以针对数据集的类型来进行改变；  提示模板，加入修饰

   2）linear probe on ResNet50（imagenet预训练冻结之后，微调分类头） 和 Zero-Shot CLIP 在多个数据集上面进行对比
   物体分类数据集上面表现良好，纹理分类，计数等较难数据集表现较差；（few-shot）

   3）few-shot 20个数据集的平均，合并  bit模型对比，表现出CLIP的迁移能力； CLIP的少样本许欸下能力相比来说更强；
   并且如果少样本学习的效果，不如零样本；当样本足够多，效果最好；原因可能是微调导致模型向这一部分数据偏移，导致效果不好。

   4）下游任务的全部数据  冻住，训练分类头；整个网络微调；   选择linear probe;彰显CLIP的优越性；且不太需要调参

   5）与efficientnet（当时在imagenet数据集表现最好的模型）进行对比，效果更好；

   6）泛化性，稳健性   数据分布偏移，很稳健；

   7）和人进行对比：五个人 zero-shot，one-shot,two-shot
   单列出来：CLIP和人对于难的任务也认为难；

5 好处：对于数据的处理减弱，不需要标注数据，只需要图片文字配对。
       文本监督训练帮助视觉模型 ，特征变成多模态特征，更便于迁移学习。
  坏处：需要大量数据，对硬件要求很高
  局限：扩大规模，还可以提高accuracy;但是对于硬件要求很高；不切实际；
     在一些数据集效果不是很好；抽象难的任务效果不好；
     数据如果很偏移，那泛化性也很差，在mnist效果也很差
     还是从给定的类别里面选择的，类别还是很确定的；
     CLIP对于数据的利用率不高，数据用量太大；数据增强，自监督，伟标签；
     数据未清洗，模型可能会有一些偏见；
     

# BLIP

1，视觉语言预训练，显著提升了任务性能；但是只适合于理解类任务（图文匹配，图文检索），生成类任务（通过图来配文字） 
   但是两边效果只能保持一个，且预训练往往依赖于较大的数据集，而且数据集来自于网上的噪声图文对，这个数据的质量难以考究。

2，CLIP针对海量数据进行预训练，这个过程中需要大量的算力和海量的数据，在本文通过CapFilt进行解决。
   利用大预言模型接入图像信息，直接提取embedding作为token输入，很难对齐且效果有限；但是如果中间插入注意力层，进行训练，需要消耗很大量的数据和计算资源
   

3，编码器和解码器的混合架构MED，较为灵活，它既可以作为单模态的编码器，又可以作为基于图像的文本编码器，或者基于图像的文本解码器。
   单模态编码器 独立的图像或者文本 图像是采用ViT，文本采用bert,通过ITC来进行对比学习，使图像和文本进行对齐
   基于图像的文本编码器，在图像编码器中每层加 跨模态注意力，文本也能够获得图片信息 开头文本序列加一个[encoder]，作为一个收集视觉特征和文本特征的交互结果。
   基于图像的文本解码器，在图像编码器中每层加 因果自注意力，用于文本生成

   BLIP 由三个视觉语言目标联合训练：图像文本的对比学习、图像文本匹配和图像条件语言建模。
   ITC（对比学习目标函数）：主要作用于视觉编码器和文本编码器，对齐特征空间。使正样本图文对的相似性更大，负样本图文对的相似性更低。使用对比学习
   ITM（图文匹配目标函数）：作用于视觉编码器和视觉文本编码器，学习图像文本的联合表征，使用一个分类头来预测图像文本对是正样本还是负样本
   LM（语言模型目标函数）：作用于视觉编码器和视觉文本编码器，目标是根据给定的图像以自回归方式来生成关于文本的描述，然后计算预测词和真实词的交叉熵损失，来进行训练

   高效利用噪声网络数据的方法:CapFilt
   Captioner（字幕器）：给一张网络图片，生成字幕。从基于图像的文本解码器初始化，然后在coco数据集上面进行微调
   Filter（过滤器）：过滤掉噪声图文对。它是一个视觉文本编码器，也从MED初始化，也进行微调，看文本是否与图像匹配
   两者结合，生成一个人工的高质量的数据对

4，视觉编码器以 ImageNet-1K 上预训练的 ViT 权重初始化，文本编码器以 BERT-Base 的权重初始化。
   
   检查CapFilt的效果：
   对比对照，只用Captiioner或者只用Filter或者都用都不用
   单独用效果会有提升，一起效果最好

   合成描述生成策略实验：
   生成方式二选一：最优路径 或者 带随机性  然后进行初始化，微调使用两种不同策略生成数据，在下游任务进行对比：说明了加入随机性对于模型的训练来说更好

   参数共享实验：
   主模型的文本编码和解码器是否共享参数，共享参数，不共享，只分开注意力层 进行对比；   
   CapFilt 共享参数 和 解耦 进行对比   解耦会导致效果更好

   对比现有的方法：
   验证泛化能力
   图文检索，图文描述，视觉问答，视觉推理，视觉对话，零样本迁移任务进行对比，说明BLIP的普适性和领先性能
   

5，架构创新，能够同时完成多任务
   新的数据生成方法，这个工作对于数据处理有很大的积极作用，提高了数据的质量。
   不足：CapFilt仍需要在COCO上面进行微调，它的效果在一定程度上取决于微调所需的人工标注数据集的质量；
        实验部分对于这个CapFilt这个模块的分析感觉没有很深入。
